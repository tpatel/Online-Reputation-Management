\documentclass[a4paper,12pt]{report} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage[parfill]{parskip}
\usepackage{url}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}

\newcommand*{\TitleFont}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{24.88}{50}%
  \selectfont}

\newcommand*{\TitleFontTwo}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{20.74}{20}%
  \selectfont}

\renewcommand\bibname{References}


\title{\TitleFontTwo DD2476: ir13 \\ \TitleFont Reputation Estimation using Twitter}


\author{Ludwig Forsberg \\ \hspace{0.5cm} \small\texttt{ludwigf@kth.se} \hspace{0.5cm} \and Romain Pomier \\ \small\texttt{romain.pomier@gmail.com} \and Kristoffer Hallqvist \\ \small\texttt{khallq@kth.se} \and Thibaut Patel \\ \small\texttt{thibaut.patel@gmail.com}}

\begin{document}

\maketitle
\clearpage

\abstract{
In this report we explore the possibility of measuring the opinion of a group of people using messages from twitter. At first we give a short introduction to the rationale of trying to analyze communication in order to extract the opinion. Then we explain why communication over twitter was used as well as exactly how the analysis, and the evaluation of the analysis, would be made. We used lexicons, Support Vector Machines and our own ``influence algorithm'' in order to perform the analysis and then we manually classified some tweets and made use of polls in order to evaluate our analysis. We explain how these components was used to both construct and evaluate our analysis, then we present the result that we got. Finally we discuss the results, present ideas of measures that could be made to improve them and some conclusions that could be made. At best we got a 58\% accuracy when it comes to classifying the tweets, and the analysis yielded a value that was a x percents off the poll whereas simply counting tweets was y percents off.
}

\clearpage
\thispagestyle{empty}
\vspace*{2.15cm}
\title{\huge \textbf{Statement of collaboration}}
\vspace{1cm}\\
\title{\large \textbf{Ludwig Forsberg}}
\begin{itemize}
    \item Implemented 6/7 of the lexicons as well as the program that combines separate lexicons into one lexicon
    \item Implemented some of the program that uses all of the different components to yield a result
    \item Implemented the tool to perform the manual classification
    \item Wrote the Introduction
    \item Wrote the Method, Lexicon, Manual and Poll part in the Method section
    \item Wrote about 1/3 of the Discussion and Conclusion section
    \item Wrote the Introduction and Method sections of the Poster
\end{itemize}
\title{\large \textbf{Romain Pomier}}
\begin{itemize}
  \item Pre-processing of words given to the lexicons and the SVM
  \item Use of the SVM (features extraction, learning, classification)
  \item Writing of the scripts that perform the tests
\end{itemize}
\title{\large \textbf{Kristoffer Hallqvist}}
\begin{itemize}
  \item item1
  \item item2
  \item item3
\end{itemize}
\title{\large \textbf{Thibaut Patel}}
\begin{itemize}
  \item item1
  \item item2
  \item item3
\end{itemize}


\tableofcontents

\clearpage
\chapter{Introduction}

\section{Background}

With the development of social networks, people tend to share more and more what they think on internet.
A lot of companies are trying to use these means to advertise and communicate directly to their customers.
They may be really interested in knowing what people are actually saying about them.
With this information, they could adapt their business strategy to be in harmony with their customers, controlling as much as possible their image.

Twitter is the dreamed place to do such an analyze of people's opinions about a company. It is a social network where user post small text messages called tweets that everybody can read. A tweet has a maximal length of 140 characters. However, such a task would be too big for a human to process, regarding the number of tweets. The only way is then to automatize the analyze with a computer.

In computer science the problem of extracting subjective information in source materials is called sentiment analysis or opinion mining. 
One of the most basic task in this field is to simply classify the polarity of a text, i.e. if it is positive or negative. 
This is sometimes called semantic orientation \cite{SenWiki}. 

The focus of the bulk of the research on automated text-analysis has rested on trying to classify separate texts.


\section{Problem Description}

In our project we were interested in examining how the classification of the text in tweets passed between people in a population actually corresponds with the populations opinion of a particular brand and explore if utilizing the number of followers, retweets and favorites, something we call ``influence'', of a tweet would better capture the opinion of the population than just using a classification of the texts. 

\chapter{Method}

To try to keep it simple we decided to estimate where in a range between negative and positive the opinion of the people would be of a particular brand.
However in order to utilize the influence of a tweet we first need to at least determine if the message itself is negative or positive, or where in the range it is.
This forces us to first perform a classification using only the text and then apply the algorithm afterwards on the result.

To perform this text-based classification we chose to use a Support Vector Machine (SVM) because that approach appeared to get the best results when we compared the results of \cite{Pang02}, \cite{Turney02} and \cite{Taboada10}.
We also decided to use a supervised approach which means that we provided the SVM with messages that already had been classified to train on.
To get these classified messages we decided to classify all the messages using lexicons and then pick a number of the messages with the highest ranking in each category for the SVM to train on.

To make sure the results from the lexicons and the SVM were reasonable we considered it necessary to evaluate them as well. 
In order to do this we manually classified a number of tweets and then used the lexicon and the SVM to classify those same tweets as well and compare the results.

To make sure the results from the manual classification also was reasonable we decided to make four different people classify the same tweets and then compare the results.

Once the SVM had made the initial classification on the text we translated it to a position in a range between negative and positive, then we applied our algorithm and received another position. Finally to evaluate both of these positions we decided to use a poll on the subject that had yielded a position on a similar range (i e the two positions could be compared).

To be able to use good polls, and get a lot of relevant tweets to process, we decided to evaluate our analysis on primarily three different well-known brands; Disney, Costco and Coca Cola.

\section{Crawler}

As we needed a considerable amount of data to train and test our algorithm, we needed a dataset of tweets. 
Given that we were also using the meta information of the tweets (number of retweet for example), we could not find pre-existing datasets which we could use.
Instead we chose to directly harvest tweets from Twitter.
We used the Twitter search as the entry point to the Twitter flow of data. 
This enabled us to get pertinent information by retrieving only the tweets containing the name of the brands we desired.

In order to reduce the scope of the project, we only considered tweets in the English language. 
The non-English tweets was filtered away while harvesting by using the Twitter language filter.
To make our task simpler we also made the assumption that there were no topic drift in a tweet. 
That is to say that if the tweet contains the name of the brand, its content is going to be about this brand.

The quantity of information extracted from Twitter corresponds to the maximum amount of tweets we could get from a search.
Twitter only maintain tweets in their indexes for 6 to 9 days.

For the 3 brands chosen, Coca Cola, Costco and Disney, we got respectively 4592, 7164 and 7403 tweets, together with the selected metadata that is used in our algorithm.

\section{Lexicon}

We used seven different lexicons one containing about 200 unique words and came from the University of Pavia \cite{WNOP}, as did another five that contained about 900 unique words and one lexicon contained about 7000 unique words. 
The last lexicon consisted of simply two lists of negative and positive words where the rest listed words with a certain negative and positive score between 0 and 1. 
The lexicons from the University of Pavia had some support for a NLP approach where some words had different scores if they were present in a text as an adjective or an adverb for example.  
But to ease the aggregation of the different lexicons we choose to keep it simple and just considered the probability of a word being adjective or adverb to be equal for example.

The lexicons were used with a simple bag-of-words model of the text so each word was sent to every lexicon, they scored the word with a value between 0 and 1 in both negativity and positivity and then returned that value. 
The score of a tweet was then created by summing up the score of all the words by all the lexicons.

We performed some pre-processing of the words by using the \textit{Natural Language Toolkit} python plugin \cite{NLTK}.
Using this plugin we removed punctuations and then we used a stemmer and a lemmatizer. This transformed all variations of ``love'', such as ``loving'', ``loves'' and ``loved'' into simply ``love'' which made it easier for the lexicon to correctly score the words.

\section{SVM}

We used a Support Vector Machine (SVM) to get the polarity of a tweet which is a supervised learning model that recognize patterns. We used the python plugin \textit{scikit-learn}\cite{Scikit} and its implementation of the SVM in our project.

Our goal was to create a SVM specialized on one brand because we believed that a word which may be considered negative for one company may not be negative for another.
For instance, the word \textit{sample} is very positive when talking about CostCo because they offer a lot of samples which people thoroughly enjoy. However, a \textit{sample} in a Disney context would probably be a sample of a movie, and then should be considered neutral.
This is the kind of informations our SVM was supposed to extract.

First we had to train our SVM and for this purpose we used our lexicons to generate training data. Of all the tweets about a brand that the lexicons labeled as ``positive'' the $n$ tweets with the highest positive score were picked. Likewise, of those labeled as ``negative'' by the lexicons the $n$ tweets with the highest negative score were picked. Finally we picked the $n$ longest tweets which the lexicon had given no negative or positive value. The SVM was then trained using these $3n$ tweets.

To represent a tweet we chose to use its uni-grams and bi-grams as features. We used the same pre-processing of the words that was used with the lexicons.

Lastly to estimate the polarity of a random tweet about a brand we simply extracted its features and classified it using the trained SVM.

\section{Algorithm and extra features}

Thanks to the previous classification we are now in possession of tweets associated with their polarity (positive, negative or neutral). Our goal is then to extract meta-data of the tweet and use this to improve the estimation of the opinion of a population.
This is the novelty aspect of our work and we chose to use this meta-data to both create our algorithm and also provide some extra features, such as presenting the top most influential tweets.

The purpose of the Algorithm is to estimate the opinion of a brand. 
This was achieved by aggregating the polarity of every tweet, weighted by the influence of the tweet over the network.
Thus the bulk of our work consists of finding the right way to measure the influence of a tweet.

The relevant meta-data we had to estimate the influence was the amount of times the tweet had been re-tweeted, the amount of followers its author had and the number of favorites it had. So we simply chose to combine them in a way felt was relevant. To get a good result the important thing was to balance the importance of each metric to each other. Since we deemed that the number of re-tweets and favorites of a tweet was about equally indicative of a an influential tweet but far more indicative then the number of followers of the author our algorithm looked a like this:

$I_t = \text{Influence of a tweet t}$, $Fo_t = \text{The number of followers of the author of tweet t}$\\
$R_t = \text{The number of re-tweets of the tweet t}$, $Fa_t = \text{The number of favorites of the tweet t}$\\
\begin{equation}
I_t = \log (Fo_t + 1) + R_t + Fa_t
\end{equation}

Once we have computed the polarity and the influence of every tweet we used the following formula to yield the final opinion position:

\begin{equation}
Opinion = 100 \times \frac{\sum_{\substack{t \in T_{pos}}} I_t}{\sum_{\substack{t \in T_{pos}}} I_t + \sum_{\substack{t \in T_{neg}}} I_t}
\end{equation}

We made the assumption here that the tweets that were classified as ``neutral'' was tweets about a brand that contained no opinion.

With the influence measurement we can now also extract the top 10 most influencial tweets. This is an easy task once you have the influence value of every tweet but this increases the actionability (the ability to use the data to perform actions) of the result. For example a company using this feature can take action by contacting the author of one of tweets and offer them some kind of deal.

\section{Manual}

To manually classify tweets we made a little script that asked the user what file the user wants to process. 
Then the user is presented with a text of a tweet and asked to classify it where the options are positive, positive but unsure, negative, negative but unsure, neutral and neutral but unsure. 
The user is free to go back and re-classify a tweet but cannot pass a tweet, to classify new tweets the last tweet presented must be classified. 
Once a number of tweets has been classified the user can save his progress in a separate file and quit. 
When the user resumes the classification process the classification will also resume where he left off.

\section{Poll}

In order to compare our results to something that measured the actual opinion of the people we decided that we more or less needed some kind of poll. 
Polls are far from perfect but should be accurate enough for our purposes. It was quite difficult to find a poll that measured the general opinion of a company. 
Most polling institutes are hesitant to disclose their raw data and instead they present their own trademarked metrics that are usually an aggregation of a wide array of different parameters.

We did however find a poll by Harris Interactive tha was made quite recently (November 13-30 in 2012) and had quite a large numbers of interviews (14 000) \cite{Harris13}. 
It also contained a section where they had a category called “Emotional Appeal” that was an aggregation of answers to how much the interviewee “Feel good about”, “Admire and Respect” and “Trust” the brand in question in a 0-7 range. 
These three scores were then summed up, divided by 21 and multiplied by 100 in order to essentially create a general score for the emotional appeal for the brand \cite{Harris13}. 
We felt this would correspond to the opinion of the population accurate enough for our purposes so we decided to use this Poll.

\chapter{Results}

\section{Manual classification}

These are the results of the manual classification of the first 100 tweets for each brand. Each member of the group individually classified the tweets, and these are the average polarities for the different brands.

\centerline{\includegraphics[scale=0.6]{../img/man1.png}}

The fact that we classified them individually led to quite a few disagreements:

\begin{itemize}
        \item \textbf{Coca Cola}: All of us agreed on 42\% of the tweets.
        \item \textbf{CostCo}: All of us agreed on 51\% of the tweets.
        \item \textbf{Disney}: All of us agreed on 34\% of the tweets.
\end{itemize}

For example, this means that out of the 100 Coca cola tweets, there were 42 which we all classified according to the same polarity while being either unsure or sure of the classification, and 58 where at least one member disagreed with the others while being either unsure or sure of the classification.

\section{Lexicon evaluation}

Using the results of our manual classification on tweets about Coca Cola, Disney and CostCo, we were able to evaluate the results of our unsupervised classification with the lexicon score on these three companies.
To classify a tweet with the lexicon, we get the positive and the negative score of this tweet, subtract one to the other, and finally apply an arbitrary threshold to get the classification.
This threshold was set to 2, so that if the score is greater than 2 the tweet is classified as positive, if the score is less than -2 the tweet is negative, and otherwise to tweet is classified neutral.

Here are the results we got on the 100 tweets we classified manually for each company:
\begin{itemize}
        \item \textbf{Coca Cola}: the lexicon agrees on 53\% of the tweets.
        \item \textbf{CostCo}: the lexicon agrees on 58\% of the tweets.
        \item \textbf{Disney}: the lexicon agrees on 58\% of the tweets.
\end{itemize}
Looking more deeply on the results, we can see that our lexicon method doesn't take a lot of risks.
Most of the mistakes consist in classifying as neutral a positive or negative tweet. That means it is unable to detect any sentimental parts in theses tweets, because they are too short, too ironic, or because the sentimental part is actually contained in the emoticons or hashtags.\\
For instance, the following tweets are classified as neutrals, while they are actually positives:\\
\centerline{\textit{Costco Sundays!!}}\\
\centerline{\textit{Need to do a Costco run today... :) \#LoveThatStore}}

\section{SVM classification}

Using the results  of our manual classification once again, we have also evaluated our SVM classifier.
We used 450 tweets to make the SVM learn, based on the method previously described.

Here are the results we got on the 100 tweets we classified manually for each company:
\begin{itemize}
        \item \textbf{Coca Cola}: the SVM agrees on 48\% of the tweets.
        \item \textbf{CostCo}: the lexicon agrees on 50\% of the tweets.
        \item \textbf{Disney}: the lexicon agrees on 16\% of the tweets.
\end{itemize}
We can see that the results are below the ones we got with the lexicon method.
That means that our SVM was not able to remove the noise in the learning data induced by the lexicon method.\\
In the case of Coca Cola and CostCo, results are pretty close to the previous ones, with around 50\% of matching.
For Disney however, results are really bad. Our guess is that as Disney has more diversified activities than the two other companies (movies, tv, amusement park, ...), the topics talked about are really different, thus the SVM would need a lot more learning datas to classify tweets correctly.

Looking closer to the results, we can see that unlike with the lexicon method, this method doesn't tend to classify as neutral when it makes mistakes.
It often classifies a negative tweet as positive and vice versa.
That means that its mistakes are more serious.


\chapter{Discussion}

\section{Lexicon}

The results from our classification using only lexicons was is good but far from great. To improve this results one may simply acquire more lexicons and use more sophisticated POS tagging.

Six out of the seven lexicons we did use came from the same source. Those six were created by applying six different machine-learning techniques on the same initial corpus. That may have led to some issues such as giving too much importance to some words only because they were present in this initial dictionary.

We may also improve our usage of the lexicons. At the moment we don't use a Part-Of-Speech tagging, so the polarity of a word is evaluated regardless of the context of the word. For example ``very good'' and ``not very good'' will more or less yield the same result. It would be good also to consider the position of the company name, so that we can see the difference between ``Costco is great but I dislike ice-cream'' and ``I dislike Costco but I think ice-cream is great''.

\section{Support Vector Machine}

The results from using the SVM are pretty poor. We suspect two main reasons for this.

First, using unigrams and bigrams as features may not be enough. To get better results, we should add more relevant features, like for example information about the urls (number, domain name, …), about the emoticons (number of positives, negatives) or about the tweet as a whole (length, number of verbs, number of adjectives etc).

Second, we trained our SVM with data that contains a lot of noise when we used the output of the lexicon to generate training data. The lexicon method offered a 50\% match on our manually classified, which doesn’t look good enough for learning purpose.

\section{Poll}

The poll that was used was an aggregation of rankings done on three different subjects (“Feel good about”, “Admire and Respect” and “Trust”). The value for these should be quite strongly correlated among themselves and also to the fact that they people who were interviewed actually “like” or “dislike” a brand. But we have no real scientific way of knowing.

We do know that the survey we used was exclusively performed in America. Unfortunately there is no way to filter tweets on the origin but one could at least filter it on language. To filter on english unfortunately does not exclude all non-americans (since it is such a global language). To make sure that the poll used is made on the same people that make the tweets one could use a poll conducted on people with a very local language and then filter on that language. The poll was conducted quite recently however.

\section{Tweaks}

Concerning the limitation of the influence computation, we couldn't find any existing result to compare the efficiency of our formula. However the resulting computation comes from tries and error when applying the function to our dataset, and seems to give good results. More research can be done in this part of our study in order to bring a deeper analysis of our results.

We are really interested in developing the actionability of our solution, that is to say that a company using our solution could take action according to the result of the program’s computation. There is several tweaks that could have been applied to our solution in order to offer a wider range of tools for a company.

As we implemented the top 10 influential tweets, we wanted to continue in the same direction by implementing the top influential Twitter users. This features enable the company to get a better reach on its customers by using these influential intermediaries. We would have used a similar “influential weight” that what we use for the influential tweets, but aggregating this measure by author.

Another extension to the current solution would be to work with the links embedded in tweets. The idea is use the content of the links that are shared on twitter about a company, and output the most important ones. We may also use the contents of these pages to detect the polarity of a tweet.

\section{Conclusion}

In the end we conclude that our results are far from perfect but that they hint at the fact that using parameters such as followers, retweets and favorites do improve the estimation of the opinion of a population over simply trying to calculate the opinion of separate tweets and then summing up the amount of tweets of each opinion.
To get a definitive answer on this matter one would have to get much better results using the lexicons and SVM in a much more sophisticated way.

\begin{thebibliography}{9}

%These references are very flexible to make however you want them, if you want them in a different format I'm open to suggestions

\bibitem{SenWiki}
  Wikipedia
  \emph{Sentiment analysis} (2013)
  \url{http://en.wikipedia.org/wiki/Sentiment_analysis} (2013/05/15)

\bibitem{Pang02}
  Bo Pang, Lillian Lee and Shivakumar Vaithyanathan\\
  \emph{Thumbs up? Sentiment classification using machine learning techniques} (2002)\\
  \url{http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf} (2013/05/14)

\bibitem{Turney02}
  Peter Turney\\
  \emph{Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews} (2002)\\
  \url{http://arxiv.org/pdf/cs/0212032v1} (2013/05/14)

\bibitem{Taboada10}
  Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll and Manfred Stede\\
  \emph{Lexicon-Based Methods for Sentiment Analysis} (2010)\\
  \url{http://cgi.sfu.ca/~mtaboada/docs/Taboada_etal_SO-CAL.pdf} (2013/05/15)

\bibitem{WNOP}
  Università degli studi di Pavia\\
  \emph{The MICRO-WNOP Corpus} (2007)\\
  \url{http://www.unipv.it/micrownop} (2013/05/15)
  
\bibitem{NLTK}
  Natural Language Toolkit, a python plugin. Website:\\
  \url{http://nltk.org/}
  
\bibitem{Scikit}
  Scikit python plugin website:\\
  \url{http://scikit-learn.org/stable/}

\bibitem{Harris13}
  Harris Interactive\\
  \emph{The Harris Poll 2013 RQ® Summary Report} (2013)\\
  \url{http://www.harrisinteractive.com/vault/2013 RQ Summary Report FINAL.pdf} (2013/05/15)

\end{thebibliography}


\end{document}
