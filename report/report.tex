\documentclass[a4paper,11pt]{report} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=2cm]{geometry}
\usepackage[parfill]{parskip}
%\usepackage[compact]{titlesec}

\newcommand*{\TitleFont}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{24.88}{50}%
  \selectfont}

\newcommand*{\TitleFontTwo}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{20.74}{20}%
  \selectfont}

\title{\TitleFontTwo DD2476: ir13 \\ \TitleFont Reputation Estimation using Twitter}

\author{Ludwig Forsberg \\ ludwigf@kth.se \and Romain Pomier \\  romain.pomier@gmail.com \and Kristoffer Hallqvist \\ khallq@kth.se \and Thibaut Patel \\ thibaut.patel@gmail.com}

\begin{document}

\maketitle
\clearpage

\abstract{
}

\tableofcontents

\clearpage
\chapter{Introduction (1p, Ludwig)}

\section{Background}

There has always been a desire to read people's minds, what they deem important and what they don't deem important, what they like and what they don't like. 
In some cases to extract the will of the people, to be able to find the best movie, adapt ones business strategy to better appeal to ones customers or for more sinister purposes such as trying to control ones inhabitants by an authoritarian government.

To directly read someones more advanced thoughts is however still far from possible (some simple thoughts can be read with an MRI). 
However how and what a person chooses to communicate do very often reflect what the person thinks. With the democratization of the written word and the information channels one can therefore try to analyze the written communication that now occurs on a huge scale to determine the opinions and ideas of people. 
The sheer volume of this information is however so great that attempting to have actual people analyzing it is infeasible. 
The only entity with the amount of processing power required are computers. 
To analyze a text-message in order to extract the thoughts of the sender is however a very daunting task for a computer, even a human that has trained and evolved tools designed to achieve this feat over thousands of years has difficulties.

In computer science the problem of extracting subjective information in source materials is called sentiment analysis or opinion mining. 
The most basic task in this field is usually to simply classify the polarity of a text, i e if it is positive or negative. 
This is sometimes called semantic orientation. 
More advanced tasks are for example classifying the emotion such as “angry”, “sad” and “happy”, and/or determining a level of “angriness” or “sadness” (as in maybe getting a score of 3 in a [0-5] interval of angriness) [a].

The focus of the bulk of the research on automated text-analysis has rested on trying to classify separate texts.

\section{Problem Description}
In our project we are interested in examining how the classification of texts in messages passed between people in a population actually corresponds with the opinion of the population and explore some methods intended to better capture the opinion of the population using the classification of texts. 

Intuitively if people are very negative about a brand for example in their messages one would assume that they have a very negative opinion of the brand, but this is not necessarily true. 

\chapter{Method (3p)}

\section{Crawler (Thibaut)}

As we need enough data to train and test our algorithm, we need a dataset of tweets. Given that we are also using the meta information of the tweets (number of retweet for example), we could not find preexisting datasets and we chose to directly harvest Twitter.
We use the Twitter search as an entry point in the Twitter flow of data. This enables us to get pertinent information by getting only the tweets containing the name of brands we choose.

We have observed in the different research papers that after getting the raw information, there is a filtering step. We chose not to do this part ourselves. In order to reduce the scope of the project, we only consider the English language. This filtering is done at the harvesting time by using the Twitter language filter.
We also assume that there is no topic drift in a tweet. That is to say if the tweet contains the name of the brand, its content is going to be about this brand.

The quantity of information extracted from Twitter corresponds to the maximum amount of tweets we can get from a search at a given time : Twitter index only the tweets from the last 6 to 9 days.

The 3 brands chosen are Coca Cola, Costco and Disney, and we got respectively 4592, 7164 and 7403 tweets, with the selected metadata that is used in our work.

Example of a tweet:

\begin{verbatim}
{
    "created_at": "Sun Apr 28 18:40:13 +0000 2013",
    "id_str": "328579420908580864",
    "text": "Shout out to Tops for filling up my frigid with Coca-Cola",
    "in_reply_to_status_id_str": null,
    "in_reply_to_user_id_str": null, 
    "retweet_count": 0,
    "favorite_count": 0,
    "user_id_str": "143882218",
    "user_screen_name": "ShutDoWnDud3_24",
    "user_followers_count": 328
}
\end{verbatim}

\section{Lexicon (Ludwig)}
We used seven different lexicons one containing about 200 unique words, five containing about 900 unique words and one containing about xxx unique words.
One of the lexicons consisted of simply two lists of negative and positive words where the rest listed words with a certain negative and positive score between 0 and 1. 
Some of these lexicons had support for an NLP approach where some words had different scores if they were present in a text as an adjective or an adverb.  
But to ease the aggregation of the different lexicons we choose to keep it simple and just considered the probability of a word being adjective or adverb to be equal for example.

The lexicons were used in simple bag-of-words model of the text so each word was sent to all lexicons, scored with a value between 0 and 1 in both negativity and positivity and then returned. 
The score of a tweet was then created by summing up the score for all the words in all the lexicons. 

\section{SVM (Romain)}
We used a Support Vector Machine (SVM) to get the polarity of a tweet. It is a supervised learning model that recognize patterns. We used the python plugin \textit{scikit-learn}\footnote{http://scikit-learn.org/stable/} and its implementation of the SVM to make it work.

Our goal is to create a SVM specialized in one company, because we believe that a word witch is polarized for one company don't have to be polarized for another.
For instance, the word \textit{sample} is positive when talking about CostCo because they offer a lot of samples and people seems to enjoy it. However, a \textit{sample} when talking about Disney would probably be a sample of a movie, and then should be neutral.
That is the king of informations our SVM is supposed to extract.

First we had to make our SVM learn. To that purpose we used our lexicon to create training data. Among all the tweets about a company, we label as "positives" the $n$ tweets with the highest positive score given by the lexicons. Likewise, we label as "negatives" the $n$ tweets with the highest negative score. Finally, we label as "neutral" the $n$ longest tweets which have both null positive and negative score. The SVM will then learn from these $3n$ tweets.

We choose as features to represent a tweet the unigrams and bigrams. We get them by cleaning the text from the punctuation, and getting all the words and couple of words. We remove all the urls.

Finally, to guess the polarity of a random tweet about the company, we extract its features, and then classify it thanks to the SVM.

\section{Tweaks (Thibaut)}

Thanks to the previous parts, we are now in possession of the tweets associated with their polarity (positive, negative or neutral). Our goal is to extract global information from this new dataset.
This is the novelty aspect in our work, and we chose to present two ways of grouping this polarity information.

What we aim for is a measure for the popularity of a brand. It is achieved by aggregating the polarity of each tweet, weighted by their influence over the network.
Then the main aspect of our work consists in finding the right way to measure the influence of a tweet.

The influence measure is critic. It is the measure that enables us to aggregate the polarity of a tweet in order to get an overall opinion on a brand.

Thanks to this measure, we can extract a first statistic : the top 10 influencial tweets. This is a simple extraction given the influence score of each tweet, but this is increasing the actionability of our results, which means that a company using this results can take action by contacting the author of those tweets for example.

The overall opinion score is our main goal, and once we have computed the polarity and the influence of each tweet, we sum the influence of the positive tweets and divide this result by the sum of the influence of the positive and negative tweets.

\section{Manual (Ludwig)}
To manually score tweets we made a little script that asked the user what file the user wants to process. 
Then the user is presented with a text and asked to classify it where the options are positive, positive but unsure, negative, negative but unsure, neutral and neutral but unsure. 
The user is free to go back and re-classify a tweet but cannot pass a tweet, to classify new tweets the last tweet presented must be classified. 
Once a number of tweets has been classified the user can save his progress in a separate file and quit. 
When the user resumes the classification process the classification will also resume where he left off.

\section{Poll (Ludwig)}
In order to compare our results to something that measured the actual opinion of the people we decided that we more or less needed some kind of poll. 
Polls are far from perfect but should be accurate enough for our purposes. It was quite difficult to find a poll that measured the general opinion of a company. 
Most polling institutes are hesitant to disclose their raw data and instead they present their own trademarked metrics that are usually an aggregation of a wide array of different parameters.

We did however find a poll by Harris Interactive made quite recently (November 13-30 in 2012) that had quite a large numbers of interviews (14 000). 
It also contained a section where they had a category called “Emotional Appeal” that was an aggregation of answers to how much the interviewee “Feel good about”, “Admire and Respect” and “Trust” the brand in question on a 0-7 scale. 
These three scores were then summed up, divided by 21 and multiplied by 100 in order to essentially create a general score for the emotional appeal for the brand [b]. 
We felt this would correspond to the opinion of the population accurate enough for our purposes.

\section{Comparing the methods (Kristoffer) [preliminary heading]}
We made three different tests to compare our different methods of determining the reputation of a brand.




Summary (Will write this in more detail soon):

Test 1 Lexicon+SVM+tweak vs Poll:
Classify all tweets using lexicon
Pick top 20 in each category
Train SVM on these
Classify with SVM
Apply tweak (generate 0-100 value (as in pos/(pos+neg))
Compare with Poll

Parameters
“influence” function (log(followers+1) + retweets + favorites)


Test 2 Manual vs Lexicon+SVM:
Classify all tweets using lexicon
Pick top 20 in each category
Train SVM on these
Classify top 100 with SVM
Compare with manual (by tweet)


Test 3 Manual vs Lexicon:
Classify first 100 tweets using lexicon
Compare with manual (by tweet)

Parameters
number of top tweet picked to train the SVM



\chapter{Results (3p, Romain and Kristoffer)}

\section{Lexicon evaluation}

Using the results of our manual classification on tweets about Coca Cola, Disney and CostCo, we were able to evaluate the results of our unsupervised classification with the lexicon score on these three companies.
To classify a tweet with the lexicon, we get the positive and the negative score of this tweet, subtract one to the other, and finally apply an arbitrary threshold to get the classification.
This threshold was set to 2, so that if the score is greater than 2 the tweet is classified as positive, if the score is less than -2 the tweet is negative, and otherwise to tweet is classified neutral.

Here are the results we got on the 100 tweets we classified manually for each company:
\begin{itemize}
        \item \textbf{Coca Cola}: the lexicon agrees on 53\% of the tweets.
        \item \textbf{CostCo}: the lexicon agrees on 58\% of the tweets.
        \item \textbf{Disney}: the lexicon agrees on 58\% of the tweets.
\end{itemize}
Looking more deeply on the results, we can see that our lexicon method doesn't take a lot of risks.
Most of the mistakes consist in classifying as neutral a positive or negative tweet. That means it is unable to detect any sentimental parts in theses tweets, because they are too short, too ironic, or because the sentimental part is actually contained in the emoticons or hashtags.\\
For instance, the following tweets are classified as neutrals, while they are actually positives:\\
\centerline{\textit{Costco Sundays!!}}\\
\centerline{\textit{Need to do a Costco run today... :) \#LoveThatStore}}

\section{SVM classification}

Using the results  of our manual classification once again, we have also evaluated our SVM classifier.
We used 450 tweets to make the SVM learn, based on the method previously described.

Here are the results we got on the 100 tweets we classified manually for each company:
\begin{itemize}
        \item \textbf{Coca Cola}: the SVM agrees on 48\% of the tweets.
        \item \textbf{CostCo}: the lexicon agrees on 50\% of the tweets.
        \item \textbf{Disney}: the lexicon agrees on 16\% of the tweets.
\end{itemize}
We can see that the results are below the ones we got with the lexicon method.
That means that our SVM was not able to remove the noise in the learning data induced by the lexicon method.\\
In the case of Coca Cola and CostCo, results are pretty close to the previous ones, with around 50\% of matching.
For Disney however, results are really bad. Our guess  is that as Disney has more diversified activities than the two other companies (movies, tv, amusement park, ...), the topics talked about are really different, thus the SVM would need a lot more learning datas to classify tweets correctly.

Looking closer to the results, we can see that unlike with the lexicon method, this method doesn't tend to classify as neutral when it makes mistakes.
It often classifies a negative tweet as positive and vice versa.
That means that its mistakes are more serious.

\chapter{Discussion (1p, all)}

\chapter{Conclusion (1p, all)}

\end{document}








