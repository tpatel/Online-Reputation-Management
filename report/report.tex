\documentclass[a4paper,12pt]{report} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage[parfill]{parskip}
\usepackage{url}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}

\newcommand*{\TitleFont}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{24.88}{50}%
  \selectfont}

\newcommand*{\TitleFontTwo}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{20.74}{20}%
  \selectfont}

\renewcommand\bibname{References}


\title{\TitleFontTwo DD2476: ir13 \\ \TitleFont Reputation Estimation using Twitter}


\author{Ludwig Forsberg \\ \hspace{0.5cm} \small\texttt{ludwigf@kth.se} \hspace{0.5cm} \and Romain Pomier \\ \small\texttt{romain.pomier@gmail.com} \and Kristoffer Hallqvist \\ \small\texttt{khallq@kth.se} \and Thibaut Patel \\ \small\texttt{thibaut.patel@gmail.com}}

\begin{document}

\maketitle
\clearpage

\abstract{
In this report we explore the possibility of measuring the opinion of a group of people, using messages from twitter. At first we give a short introduction to the rationale of trying to analyze communication in order to extract the opinion. Then we explain why communication over twitter was used as well as exactly how the analysis, and the evaluation of the analysis, was made. We used lexicons, a Support Vector Machine and our own ``influence'' algorithm in order to perform the analysis. Then we manually classified some tweets and compared the results to a poll results in order to evaluate them. We explain how these components were used to both construct and evaluate our analysis, and then we present the results that we got. Finally we discuss the results, present some measures that could improve them and some conclusions that could be made. The analysis with the influence algorithm yielded a value that was on average 74.67 units off the poll whereas simply counting tweets without the influence algorithm was on average 48.96 units off on a 0 to 100 scale.
}

\clearpage
\thispagestyle{empty}
\vspace*{2.15cm}
\title{\huge \textbf{Statement of collaboration}}
\vspace{1cm}\\
\title{\large \textbf{Ludwig Forsberg}}
\begin{itemize}
    \item Implemented 6/7 of the lexicons as well as the program that combines separate lexicons into one lexicon
    \item Implemented some of the program that uses all of the different components to yield a result
    \item Implemented the tool to perform the manual classification
    \item Wrote the Introduction and the Method, Lexicon, Manual and Poll part in the Method section and a large part of the Discussion and Conclusion section
    \item Wrote the Introduction, Method and Discussion \& Conclusion sections of the Poster
\end{itemize}
\title{\large \textbf{Romain Pomier}}
\begin{itemize}
  \item Pre-processing of words given to the lexicons and the SVM
  \item Use of the SVM (features extraction, learning, classification)
  \item Writing of the scripts that perform the tests
\end{itemize}
\title{\large \textbf{Kristoffer Hallqvist}}
\begin{itemize}
  \item Script for extracting manual classification scores
  \item Statistical analysis
  \item Production of charts and graphs
\end{itemize}
\title{\large \textbf{Thibaut Patel}}
\begin{itemize}
  \item Dataset construction, using the Twitter API and applying transformation to the resulting data.
  \item Design and implementation of the influence measure and the opinion aggregation (the algorithm).
  \item Participation in the poster: Overview diagram and Algorithm formulas.
\end{itemize}
\title{\large \textbf{Everyone}}
\begin{itemize}
  \item Project planning
  \item Report writing
\end{itemize}


\tableofcontents

\clearpage
\chapter{Introduction}

\section{Background}
With the development of social networks, people are sharing their opinions over the Internet more and more.
A lot of companies are trying to use these means to advertise and communicate directly to their customers.
They may be really interested in knowing people's opinion of their brand.
With this information, they could adapt their business strategy to be in harmony with their customers, taking more control over their image.

Twitter \cite{TwitterAbout} is the ideal place for analyzing people's opinions of a brand. 
It is a social network where user post small text messages called tweets that everybody can read. 
A tweet has a maximal length of 140 characters. However, such a task would be too big for a human to process, considering the number of tweets.
The only realistic way is then to automatize the analysis using a computer.

In computer science the problem of extracting subjective information in source materials is called sentiment analysis or opinion mining. 
One of the most basic tasks in this field is to simply classify the polarity of a text, i.e. determining whether it is positive or negative. 
This is sometimes called semantic orientation \cite{SenWiki}. 

Most of the research done within the automated text-analysis field has focused on trying to classify texts without utilizing its meta-data.

\section{Problem Description}

In our project we were interested in building a classifier that can extract the polarity of messages exchanged on Twitter that are about one specific company. Using the informations given by the website, we measure the ``influence'' of the tweets, and then aggregate the results to get the opinion of the population about the company.

\chapter{Method}

To try to keep it simple, we decided to estimate the fraction of positive opinions for a particular brand.
However in order to utilize the influence of a tweet we first need to at least determine if the message itself is negative or positive.
This forces us to first perform a classification using only the text and then apply the algorithm on the results afterwards.

To perform this text-based classification we chose to use a Support Vector Machine (SVM) because that approach appeared to give the best results when we compared the results of \cite{Pang02}, \cite{Turney02} and \cite{Taboada10}.
We also decided to use a distant supervised approach which means that to make the SVM learn we provide it messages that are classified automatically with an unsupervised method.
We decided to classify all the messages using lexicons and then pick a number of the messages with the highest ranking in each category for the SVM to train on.

To make sure the results from the lexicons and the SVM were reasonable, we considered it necessary to evaluate them as well. 
In order to do this, we manually classified a number of tweets and then used the lexicon and the SVM to classify those tweets as well, and compared the results.

To balance the results from the manual classification, we decided to let all four different group members classify the same tweets and then compare the results.

Once the SVM had made the initial classification, we simply estimated the fraction of positive opinions for a particular brand by counting the tweets. After that we applied our algorithm to get a weighted fraction, and then to evaluate these we compared them to a poll result.

To be able to use good polls, and get a lot of relevant tweets to process, we decided to evaluate our analysis on primarily three different well-known brands; Disney, Costco and Coca Cola.

\section{Crawler}

As we needed a considerable amount of data to train and test our algorithm, we needed a dataset of tweets. 
Given that we were also using the meta-information of the tweets (number of re-tweet for example), we could not find pre-existing datasets which we could use.
Instead we chose to directly harvest tweets from Twitter.
We used the Twitter search as the entry point to the Twitter flow of data. 
This enabled us to get relevant information by retrieving only the tweets containing the name of the brands we desired.

In order to reduce the scope of the project, we only considered tweets in the English language. 
The non-English tweets were filtered away while harvesting, by using the Twitter language filter.
To make our task simpler, we also made the assumption that there were no topic drifts in a tweet;
that is to say that if the tweet contains the name of the brand, its content is going to be about this brand.

The quantity of information extracted from Twitter corresponds to the maximum amount of tweets we could get from a search.
Twitter only maintain tweets in their indexes for 6 to 9 days.\cite{Twitter}

For the 3 brands chosen: Coca Cola, Costco and Disney, we got 4592, 7164 and 7403 tweets respectively, together with the metadata that is used in our algorithm.

\section{Lexicon}

We used seven different lexicons: one containing about 200 unique words, as well as another five that contained about 900 unique words, all from the University of Pavia \cite{WNOP07}, and also one lexicon from \cite{Liu04} containing about 7000 unique words.
The last lexicon simply consisted of two lists of negative and positive words respectively, whereas the other ones listed words with a certain negative and positive score between 0 and 1, inclusive. 
The lexicons from the University of Pavia had some support for an NLP approach where some words had different scores if they were present in a text as an adjective or an adverb for example.  
But to ease the aggregation of the different lexicons, we choose to keep it simple and just considered the probability of a word being an adjective or an adverb to be equal, for example.

The lexicons were used with a simple bag-of-words model of the text so each word was sent to every lexicon, they scored the word with a value between 0 and 1 in both negativity and positivity and then returned that value. 
The score of a tweet was then created by summing up the positive and negative scores, respectively, of all the words by all the lexicons.

We performed some pre-processing of the words by using the \textit{Natural Language Toolkit} python plugin \cite{NLTK}.
Using this plugin we removed punctuations, and then we used a stemmer and a lemmatizer. This transformed all variations of ``love'', such as ``loving'', ``loves'' and ``loved'' into simply ``love'' which made it easier for the lexicon to correctly grade the words.

\section{Support Vector Machine}

We used a Support Vector Machine (SVM) - which is a supervised learning model that recognizes patterns - to get the polarity of a tweet. We used the python plugin Scikit-learn\cite{Scikit} and its implementation of the SVM in our project.

Our goal was to create an SVM specialized on one brand because we believed that a word which may be considered negative for one brand may not be negative for another.
For instance, the word \textit{sample} is very positive when talking about Costco because they offer a lot of samples which people thoroughly enjoy. However, a \textit{sample} in a Disney context would probably be a sample of a movie, and then should be considered neutral.
This is the kind of information our SVM was supposed to extract.

First we had to train our SVM and for this purpose we used our lexicons to generate training data. Of all the tweets about a brand that the lexicons labeled as ``positive'', the $n$ tweets with the highest positive score were picked. Likewise, of those labeled as ``negative'' by the lexicons the $n$ tweets with the highest negative score were picked. Finally we picked the $n$ longest tweets which the lexicon had given no negative or positive value. The SVM was then trained using these $3n$ tweets.

We used the unigrams and bigrams of a tweet as features in the SVM, i.e. the things used to represent the tweet. We used the same pre-processing of the words that was used with the lexicons.

Lastly, to estimate the polarity of a random tweet about a brand we simply extracted its features and classified it using the trained SVM.

\section{Algorithm and extra features}

Thanks to the previous classification we are now in possession of tweets associated with their polarity (positive, negative or neutral). Our goal is then to extract meta-data of the tweet and let it affect our results.
This is the novelty aspect of our work and we chose to use this meta-data to both create our algorithm and also provide some extra features, such as presenting the top most influential tweets.

The purpose of the algorithm is to attempt to improve an estimation of the opinion of a brand. 
This was done by aggregating the polarity of every tweet, weighted by the influence of the tweet over the network.
The bulk of our work thus consists of finding the right way to measure the influence of a tweet.

We assumed that a tweet which has more re-tweets, more favorites and whose author has more followers would better indicate the opinion of the whole population than one with fewer of them. Therefore, we chose to combine these metrics and use them as weights when evaluating the opinion of the people. Since we deemed that the number of re-tweets and favorites of a tweet to be about equally indicative of an influential tweet but far more indicative then the number of followers of the author, our algorithm looked as follows:

$I_t = \text{Influence of a tweet t}$\\
$Fo_t = \text{The number of followers of the author of tweet t}$\\
$R_t = \text{The number of re-tweets of the tweet t}$\\
$Fa_t = \text{The number of favorites of the tweet t}$\\
\begin{equation}
I_t = \log (Fo_t + 1) + R_t + Fa_t
\end{equation}

Once we have computed the polarity and the influence of every tweet we used the following formula to yield the final opinion:\\
$T_{pos} = \text{The set of positive tweets}$\\
$T_{neg} = \text{The set of negative tweets}$\\
\begin{equation}
Opinion = 100 \times \frac{\sum_{\substack{t \in T_{pos}}} I_t}{\sum_{\substack{t \in T_{pos}}} I_t + \sum_{\substack{t \in T_{neg}}} I_t}
\end{equation}

We made the assumption here that the tweets that were classified as ``neutral'' were tweets that contained no opinion about the brand.

With the influence measurement, we can now also extract the top ten most influential tweets. This is an easy task once you have the influence value of every tweet, but this increases the actionability (the ability to use the data to perform actual actions) of the result. For example, a company using this feature can take action by contacting the author of one of tweets and offer them some kind of deal.


\section{Manual classification}

To manually classify the tweets, we made a little script.
The user is presented with a text of a tweet and asked to classify it where the options are: positive, positive but unsure, negative, negative but unsure, neutral and neutral but unsure.

\section{Poll}

In order to compare our results to something that measured the actual opinion of the people, we decided that we needed some kind of poll result. 
Polls are far from perfect but should be accurate enough for our purposes. It was quite difficult to find a poll that measured the general opinion of a brand. 
Most polling institutes are hesitant to disclose their raw data and instead they present their own trademarked metrics that are usually an aggregation of a wide array of different parameters.

We did however find a poll by Harris Interactive that was made quite recently (November 13-30 in 2012) and had quite a large numbers of interviews (14 000) \cite{Harris13}. 
It also contained a section where they had a category called “Emotional Appeal” that was an aggregation of answers to how much the interviewee “Feel good about”, “Admire and Respect” and “Trust” the brand in question in a 0-7 scoring range. 
These three scores were then summed up, divided by 21 and multiplied by 100 in order to essentially create a general score for the emotional appeal for the brand \cite{Harris13}. 
We felt this would correspond to the opinion of the population accurately enough for our purposes, so we decided to use it.

\chapter{Results}

\section{Manual classification}

These are the results of the manual classification of the first 100 tweets for each brand. Each member of the group individually classified the tweets as either positive, negative or neutral, and these are the average polarities for the different brands:

\centerline{\includegraphics[scale=0.55]{../img/man1.png}}

The fact that we classified them individually led to quite a few disagreements:

\begin{itemize}
        \item \textbf{Coca Cola}: All of us agreed on 42\% of the tweets.
        \item \textbf{CostCo}: All of us agreed on 51\% of the tweets.
        \item \textbf{Disney}: All of us agreed on 34\% of the tweets.
\end{itemize}

For example, this means that out of the 100 Coca cola tweets, there were 42 which we all classified according to the same polarity and 58 where at least one member disagreed with the others. Confidence is not taken into consideration.

\section{Lexicon evaluation}

Using the results of our manual classification on tweets about Coca Cola, Disney and CostCo, we were able to evaluate the results of our unsupervised classification with the lexicon score on these three brands.
To classify a tweet with the lexicon method, we take the positive score of the tweet, subtract the negative score and finally apply an arbitrary threshold to determine the classification.
This threshold was set to 2, which means that if the score is greater than 2 the tweet is classified as positive, if the score is less than -2 the tweet is negative, and otherwise the tweet is classified neutral.

Here are the results we got on the 100 tweets we classified manually for each brand:
\begin{itemize}
        \item \textbf{Coca Cola}: the lexicon agrees on 53\% of the tweets.
        \item \textbf{CostCo}: the lexicon agrees on 58\% of the tweets.
        \item \textbf{Disney}: the lexicon agrees on 58\% of the tweets.
\end{itemize}

\section{SVM classification}

We evaluated our SVM classifier in much the same way as the lexicon method, i.e. by comparing them to the results of our manual classification.
We used 450 tweets to train the SVM, based on the method previously described.

Here are the results we got for the 100 tweets we classified manually for each brand:
\begin{itemize}
        \item \textbf{Coca Cola}: the SVM agrees on 48\% of the tweets.
        \item \textbf{CostCo}: the SVM agrees on 50\% of the tweets.
        \item \textbf{Disney}: the SVM agrees on 16\% of the tweets.
\end{itemize}

\section{Final scores}
Here we can see the results achieved through the different methods by our program, compared to the poll:


\centerline{\includegraphics[scale=0.55]{../img/full1.png}}


The results from the SVM and lexicon were calculated by dividing the number of positive tweets with the total number of non-neutral tweets.

\chapter{Discussion}

\section{Lexicon}

The overall results from our classification using only the lexicons are far from great. 

From looking at how separate tweets were actually classified we noticed that the most common mistake the lexicons made was to classify a positive or negative tweet as neutral. 
This means that it is unable to detect the parts in these tweets that signal opinions, either because they are not present in the lexicon, because they are too short, too ironic, or because they are contained in an emoticon or hashtag.
For instance, the following tweets are classified as neutral, even though they are arguably positive:
\\
\centerline{\textit{Costco Sundays!!}}\\
\centerline{\textit{Need to do a Costco run today... :) \#LoveThatStore}}

To reduce this problem, one may add more lexicons and use lexicons from more sources. Six out of the seven lexicons we did use came from the same source. Those six were created by applying six different machine-learning techniques on the same initial corpus. That may have led to some issues such as giving too much significance to some words only because they were present in this initial dictionary.

Anoter measure to improve the result may be to make use of some Part-Of-Speech tagging, so the polarity of a word is evaluated using the context of the word. For example \emph{``very good''} and \emph{``not very good''} will more or less yield the same result. It would be good also to consider the position of the brand name, so that we can see the difference between \emph{``Costco is great but I dislike ice-cream''} and \emph{``I dislike Costco but I think ice-cream is great''}.

We did perform some lemmatization but to improve the results one would have to make use of even more sophisticated lemmatization.

\section{Support Vector Machine}

Interestingly enough, we can see that the results from the SVM worse than those from the lexicons.
This probably means that our SVM was not able to remove the noise in the training data induced by the lexicon method.
In the case of Coca Cola and CostCo, the results are pretty close to lexicon, with around 50\% of matching.
For Disney however, results are really bad. 
Our guess is that as Disney is a much more diversified company than the two others (they make movies, tv, run an amusement park, etc). The topics touched upon in those tweets are really different, indicating that the SVM would need a lot more training data to classify those tweets more accurately.

If one looks closer at how the SVM actually classified separate tweets, we can see that unlike the lexicon it doesn't have a bias toward classifiying a tweet as neutral; instead it often classifies a negative tweet as positive and vice versa.
In essence, this means that its mistakes are more serious.

We believe there are two major reasons for the poor performance of the SVM.

Firstly, using unigrams and bigrams to represent a tweet may not be enough. To get better results, one should probably add more relevant features, like for example information about the URLs (number, domain name, etc), about the emoticons (number of positives, negatives) or about the tweet as a whole (length, number of verbs, number of adjectives etc).


Secondly, we trained our SVM with data that contained a lot of noise when we used the output of the lexicons to generate training data. The lexicons had a 50\% accuracy rate when classifying the tweets we classified manually, and this is probably way too noisy for training purposes.

\section{Poll}

The poll that was used was an aggregation of rankings done on three different subjects (“Feel good about”, “Admire and Respect” and “Trust”). The value of these should be quite strongly correlated among themselves and also correlated to whether the people who were interviewed actually “like” or “dislike” a brand. But we have no real scientific way of knowing this.

We do know that the survey we used was exclusively performed in America. Unfortunately there is no way to filter tweets on the origin but one could at least filter it on language. To filter on English unfortunately does not exclude all non-Americans (since it is such a global language). To make sure that the poll used is made on the same people that make the tweets one could use a poll conducted on people with a very local language and then filter on that language. The poll was conducted quite recently, though.

However, if one compares the poll with the manual classification, the difference is on average only 3.77 percentage points. The data from the manual classification is way too small to make any definitive conclusions but at least it hints at the fact that the poll was quite accurate.

\section{Algorithm and features}

The computation of our algorithm ultimately comes from assumptions made on how a twitter user reacts to a tweet depending on the opinion of the user and the opinion voiced in the tweet. Our assumptions were that if the user re-tweets the tweet, favorites the tweet or is the follower of someone who made the tweet then the probability is higher that the user supports the opinion rather than disagrees with it. Intuitively this seems to be correct but judging from our results this seems not to be the case. The results instead rather indicate that at least one of these assumptions must be wrong since applying the algorithm seems to make the results worse, or rather, one of the parameters seems to have an adverse effect. 

If one looks at how the different values affect the ``influence-weight'' the algorithm generates for each tweet, we see that it is the re-tweets factor that dominate the others. So it is likely that the assumption we made about the re-tweeting is the problem.

Apart from the actual algorithm, we were also really interested in developing the actionability of our analysis. There are several features that could have been applied to our solution in order to offer a wider range of tools for a company.

As we implemented the top ten influential tweets, we wanted to continue in the same direction and implement the top influential Twitter users as well. This feature would enable a company to easier reach out to its customers. We would have used a similar “influential weight” that what used for the influential tweets, but aggregating this measure by author instead.

Another extension to the current analysis would be to work with the links embedded in tweets. The idea is use the content of the links that are shared on twitter about a company, and output the most important ones. We may also use the contents of these pages to detect the polarity of a tweet.

\section{Conclusion}

In the end we conclude that our results are far from perfect and that more tests and a more sophisticated method is required to reach a definitive answer. However the results hints at using our influence algorithm actually makes the results worse. If this is true then it means that at least one of our assumptions when creating the algorithm was incorrect. A tweet with either more re-tweets, favorites or followers of its author actually reflects the opinion of the population less accurately than a tweet with fewer of those. Perhaps people tend to re-tweet tweets they find interesting, yet in effect have an opposite opinion.

\begin{thebibliography}{12}

%These references are very flexible to make however you want them, if you want them in a different format I'm open to suggestions
\vspace{-1cm}
\bibitem{TwitterAbout}
  Twitter
  \emph{The fastest, simplest way to stay close to everything you care about} (2013)
  \url{https://twitter.com/about} (2013/05/15)

\bibitem{SenWiki}
  Wikipedia
  \emph{Sentiment analysis} (2013)
  \url{http://en.wikipedia.org/wiki/Sentiment_analysis} (2013/05/15)

\bibitem{Pang02}
  Bo Pang, Lillian Lee and Shivakumar Vaithyanathan\\
  \emph{Thumbs up? Sentiment classification using machine learning techniques} (2002)\\
  \url{http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf} (2013/05/14)

\bibitem{Turney02}
  Peter Turney\\
  \emph{Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews} (2002)\\
  \url{http://arxiv.org/pdf/cs/0212032v1} (2013/05/14)

\bibitem{Taboada10}
  Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll and Manfred Stede\\
  \emph{Lexicon-Based Methods for Sentiment Analysis} (2010)\\
  \url{http://cgi.sfu.ca/~mtaboada/docs/Taboada_etal_SO-CAL.pdf} (2013/05/15)

\bibitem{Twitter}
  Twitter Search API usage. Website:\\
  \url{https://dev.twitter.com/docs/using-search}

\bibitem{WNOP07}
  Università degli studi di Pavia\\
  \emph{The MICRO-WNOP Corpus} (2007)\\
  \url{http://www.unipv.it/micrownop} (2013/05/15)
  
\bibitem{Liu04}
  Bing Liu and and Minqing Hu\\
  \emph{A list of positive and negative opinion words or sentiment words for English} (2004)
  \url{http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar} (2013/05/16)

\bibitem{NLTK}
  Natural Language Toolkit, a python plugin. Website:\\
  \url{http://nltk.org/}
  
\bibitem{Scikit}
  Scikit python plugin website:\\
  \url{http://scikit-learn.org/stable/}

\bibitem{Harris13}
  Harris Interactive\\
  \emph{The Harris Poll 2013 RQ® Summary Report} (2013)\\
  \url{http://www.harrisinteractive.com/vault/2013 RQ Summary Report FINAL.pdf} (2013/05/15)

\end{thebibliography}


\end{document}
