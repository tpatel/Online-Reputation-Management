\documentclass[a4paper,12pt]{report} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage[parfill]{parskip}
\usepackage{url}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}

\newcommand*{\TitleFont}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{24.88}{50}%
  \selectfont}

\newcommand*{\TitleFontTwo}{%
  \usefont{\encodingdefault}{\rmdefault}{b}{n}%
  \fontsize{20.74}{20}%
  \selectfont}

\renewcommand\bibname{References}


\title{\TitleFontTwo DD2476: ir13 \\ \TitleFont Reputation Estimation using Twitter}


\author{Ludwig Forsberg \\ \hspace{0.5cm} \small\texttt{ludwigf@kth.se} \hspace{0.5cm} \and Romain Pomier \\ \small\texttt{romain.pomier@gmail.com} \and Kristoffer Hallqvist \\ \small\texttt{khallq@kth.se} \and Thibaut Patel \\ \small\texttt{thibaut.patel@gmail.com}}

\begin{document}

\maketitle
\clearpage

\abstract{
In this report we explore the possibility of measuring the opinion of a group of people, using messages from twitter. At first we give a short introduction to the rationale of trying to analyze communication in order to extract the opinion. Then we explain why communication over twitter was used as well as exactly how the analysis, and the evaluation of the analysis, was made. We used lexicons, a Support Vector Machine and our own ``influence'' algorithm in order to perform the analysis and then we manually classified some tweets and made use of a poll in order to evaluate it. We explain how these components were used to both construct and evaluate our analysis, then we present the result that we got. Finally we discuss the results, present some measures that could improve them and some conclusions that could be made. The analysis with the influence algorithm yielded a value that was on average 74.67 units off the poll whereas simply counting tweets without the influence algorithm was on average 48.96 units off on a 0 to 100 scale.
}

\clearpage
\thispagestyle{empty}
\vspace*{2.15cm}
\title{\huge \textbf{Statement of collaboration}}
\vspace{1cm}\\
\title{\large \textbf{Ludwig Forsberg}}
\begin{itemize}
    \item Implemented 6/7 of the lexicons as well as the program that combines separate lexicons into one lexicon
    \item Implemented some of the program that uses all of the different components to yield a result
    \item Implemented the tool to perform the manual classification
    \item Wrote the Introduction and the Method, Lexicon, Manual and Poll part in the Method section and a large part of the Discussion and Conclusion section
    \item Wrote the Introduction, Method and Discussion \& Conclusion sections of the Poster
\end{itemize}
\title{\large \textbf{Romain Pomier}}
\begin{itemize}
  \item Pre-processing of words given to the lexicons and the SVM
  \item Use of the SVM (features extraction, learning, classification)
  \item Writing of the scripts that perform the tests
\end{itemize}
\title{\large \textbf{Kristoffer Hallqvist}}
\begin{itemize}
  \item Script for extracting manual classification scores
  \item Statistical analysis
  \item Production of charts and graphs
\end{itemize}
\title{\large \textbf{Thibaut Patel}}
\begin{itemize}
  \item Development of the Algorithm and extra features
  \item Development of the Algorithm and extra features
  \item Wrote the Algorithm and 
  \item item3
\end{itemize}
\title{\large \textbf{Everyone}}
\begin{itemize}
  \item Project planning
  \item Report writing
\end{itemize}


\tableofcontents

\clearpage
\chapter{Introduction}

\section{Background}
With the development of social networks, people are sharing their opinions over the Internet more and more.
A lot of companies are trying to use these means to advertise and communicate directly to their customers.
They may be really interested in knowing people's opinion of their brand.
With this information, they could adapt their business strategy to be in harmony with their customers, taking more control over their image.

Twitter \cite{Twitter} is the ideal place for analyzing people's opinions of a brand. 
It is a social network where user post small text messages called tweets that everybody can read. 
A tweet has a maximal length of 140 characters. However, such a task would be too big for a human to process, considering the number of tweets.
The only realistic way is then to automatize the analysis using a computer.

In computer science the problem of extracting subjective information in source materials is called sentiment analysis or opinion mining. 
One of the most basic tasks in this field is to simply classify the polarity of a text, i.e. determining whether it is positive or negative. 
This is sometimes called semantic orientation \cite{SenWiki}. 

Most of the research done within the automated text-analysis field has focused on trying to classify texts without utilizing its meta-data.

\section{Problem Description}

In our project we were interested in examining how the classification of the text in tweets passed between people in a population actually corresponds with the populations opinion of a particular brand and explore if utilizing the number of followers, retweets and favorites, something we call ``influence'', of a tweet would better capture the opinion of the population than just using a classification of the texts. 

\chapter{Method}

To try to keep it simple, we decided to estimate the fraction of positive opinions for a particular brand.
However in order to utilize the influence of a tweet we first need to at least determine if the message itself is negative or positive.
This forces us to first perform a classification using only the text and then apply the algorithm afterwards on the result.

To perform this text-based classification we chose to use a Support Vector Machine (SVM) because that approach appeared to get the best results when we compared the results of \cite{Pang02}, \cite{Turney02} and \cite{Taboada10}.
We also decided to use a supervised approach which means that we provided the SVM with messages that already had been classified to train on.
To get these classified messages we decided to classify all the messages using lexicons and then pick a number of the messages with the highest ranking in each category for the SVM to train on.

To make sure the results from the lexicons and the SVM were reasonable we considered it necessary to evaluate them as well. 
In order to do this we manually classified a number of tweets and then used the lexicon and the SVM to classify those same tweets as well and compare the results.

To make sure the results from the manual classification also was reasonable we decided to make four different people classify the same tweets and then compare the results.

Once the SVM had made the initial classification, we simply estimated the fraction of positive opinions for a particular brand by counting the tweets. After that we applied our algorithm to get a weighted fraction, and then to evaluate these we compared them to a poll result.

To be able to use good polls, and get a lot of relevant tweets to process, we decided to evaluate our analysis on primarily three different well-known brands; Disney, Costco and Coca Cola.

\section{Crawler}

As we needed a considerable amount of data to train and test our algorithm, we needed a dataset of tweets. 
Given that we were also using the meta information of the tweets (number of retweet for example), we could not find pre-existing datasets which we could use.
Instead we chose to directly harvest tweets from Twitter.
We used the Twitter search as the entry point to the Twitter flow of data. 
This enabled us to get pertinent information by retrieving only the tweets containing the name of the brands we desired.

In order to reduce the scope of the project, we only considered tweets in the English language. 
The non-English tweets was filtered away while harvesting by using the Twitter language filter.
To make our task simpler we also made the assumption that there were no topic drift in a tweet. 
That is to say that if the tweet contains the name of the brand, its content is going to be about this brand.

The quantity of information extracted from Twitter corresponds to the maximum amount of tweets we could get from a search.
Twitter only maintain tweets in their indexes for 6 to 9 days.

For the 3 brands chosen, Coca Cola, Costco and Disney, we got respectively 4592, 7164 and 7403 tweets, together with the selected metadata that is used in our algorithm.

\section{Lexicon}

We used seven different lexicons one containing about 200 unique words and came from the University of Pavia \cite{WNOP07}, as did another five that contained about 900 unique words and one lexicon contained about 7000 unique words from \cite{Liu04}. 
The last lexicon consisted of simply two lists of negative and positive words where the rest listed words with a certain negative and positive score between 0 and 1. 
The lexicons from the University of Pavia had some support for a NLP approach where some words had different scores if they were present in a text as an adjective or an adverb for example.  
But to ease the aggregation of the different lexicons we choose to keep it simple and just considered the probability of a word being adjective or adverb to be equal for example.

The lexicons were used with a simple bag-of-words model of the text so each word was sent to every lexicon, they scored the word with a value between 0 and 1 in both negativity and positivity and then returned that value. 
The score of a tweet was then created by summing up the positive and negative score of all the words by all the lexicons.

We performed some pre-processing of the words by using the \textit{Natural Language Toolkit} python plugin \cite{NLTK}.
Using this plugin we removed punctuations and then we used a stemmer and a lemmatizer. This transformed all variations of ``love'', such as ``loving'', ``loves'' and ``loved'' into simply ``love'' which made it easier for the lexicon to correctly score the words.

\section{Support Vector Machine}

We used a Support Vector Machine (SVM) to get the polarity of a tweet which is a supervised learning model that recognize patterns. We used the python plugin \textit{scikit-learn}\cite{Scikit} and its implementation of the SVM in our project.

Our goal was to create a SVM specialized on one brand because we believed that a word which may be considered negative for one brand may not be negative for another.
For instance, the word \textit{sample} is very positive when talking about CostCo because they offer a lot of samples which people thoroughly enjoy. However, a \textit{sample} in a Disney context would probably be a sample of a movie, and then should be considered neutral.
This is the kind of informations our SVM was supposed to extract.

First we had to train our SVM and for this purpose we used our lexicons to generate training data. Of all the tweets about a brand that the lexicons labeled as ``positive'' the $n$ tweets with the highest positive score were picked. Likewise, of those labeled as ``negative'' by the lexicons the $n$ tweets with the highest negative score were picked. Finally we picked the $n$ longest tweets which the lexicon had given no negative or positive value. The SVM was then trained using these $3n$ tweets.

To represent a tweet we chose to use its uni-grams and bi-grams as features. We used the same pre-processing of the words that was used with the lexicons.

Lastly to estimate the polarity of a random tweet about a brand we simply extracted its features and classified it using the trained SVM.

\section{Algorithm and extra features}

Thanks to the previous classification we are now in possession of tweets associated with their polarity (positive, negative or neutral). Our goal is then to extract meta-data of the tweet and use this to improve the estimation of the opinion of a population.
This is the novelty aspect of our work and we chose to use this meta-data to both create our algorithm and also provide some extra features, such as presenting the top most influential tweets.

The purpose of the Algorithm is to improve an estimation of the opinion of a brand. 
This was achieved by aggregating the polarity of every tweet, weighted by the influence of the tweet over the network.
Thus the bulk of our work consists of finding the right way to measure the influence of a tweet.

We assumed that a tweet which has more re-tweets, more favorites and whose author has more followers would better indicate the opinion of the whole population then one with less. Therefore we chose to combine these metrics and then use them as weights when evaluating the opinion of the people. Since we deemed that the number of re-tweets and favorites of a tweet to be about equally indicative of a an influential tweet but far more indicative then the number of followers of the author our algorithm looked as follows:

$I_t = \text{Influence of a tweet t}$\\
$Fo_t = \text{The number of followers of the author of tweet t}$\\
$R_t = \text{The number of re-tweets of the tweet t}$\\
$Fa_t = \text{The number of favorites of the tweet t}$\\
\begin{equation}
I_t = \log (Fo_t + 1) + R_t + Fa_t
\end{equation}

Once we have computed the polarity and the influence of every tweet we used the following formula to yield the final opinion position:\\
$T_{pos} = \text{The set of positive tweets}$\\
$T_{neg} = \text{The set of negative tweets}$\\
\begin{equation}
Opinion = 100 \times \frac{\sum_{\substack{t \in T_{pos}}} I_t}{\sum_{\substack{t \in T_{pos}}} I_t + \sum_{\substack{t \in T_{neg}}} I_t}
\end{equation}

We made the assumption here that the tweets that were classified as ``neutral'' was tweets about a brand that contained no opinion.

With the influence measurement we can now also extract the top ten most influencial tweets. This is an easy task once you have the influence value of every tweet but this increases the actionability (the ability to use the data to perform actual actions) of the result. For example a company using this feature can take action by contacting the author of one of tweets and offer them some kind of deal.


\section{Manual}

To manually classify tweets we made a little script that asked the user what file the user wants to process. 
Then the user is presented with a text of a tweet and asked to classify it where the options are positive, positive but unsure, negative, negative but unsure, neutral and neutral but unsure. 
The user is free to go back and re-classify a tweet but cannot pass a tweet, to classify new tweets the last tweet presented must be classified. 
Once a number of tweets has been classified the user can save his progress in a separate file and quit. 
When the user resumes the classification process the classification will also resume where he left off.

\section{Poll}

In order to compare our results to something that measured the actual opinion of the people we decided that we more or less needed some kind of poll. 
Polls are far from perfect but should be accurate enough for our purposes. It was quite difficult to find a poll that measured the general opinion of a brand. 
Most polling institutes are hesitant to disclose their raw data and instead they present their own trademarked metrics that are usually an aggregation of a wide array of different parameters.

We did however find a poll by Harris Interactive tha was made quite recently (November 13-30 in 2012) and had quite a large numbers of interviews (14 000) \cite{Harris13}. 
It also contained a section where they had a category called “Emotional Appeal” that was an aggregation of answers to how much the interviewee “Feel good about”, “Admire and Respect” and “Trust” the brand in question in a 0-7 range. 
These three scores were then summed up, divided by 21 and multiplied by 100 in order to essentially create a general score for the emotional appeal for the brand \cite{Harris13}. 
We felt this would correspond to the opinion of the population accurate enough for our purposes so we decided to use this Poll.

\chapter{Results}

\section{Manual classification}

These are the results of the manual classification of the first 100 tweets for each brand. Each member of the group individually classified the tweets, and these are the average polarities for the different brands.

\centerline{\includegraphics[scale=0.45]{../img/man1.png}}
\vspace{-0.7cm}

The fact that we classified them individually led to quite a few disagreements:

\begin{itemize}
        \item \textbf{Coca Cola}: All of us agreed on 42\% of the tweets.
        \item \textbf{CostCo}: All of us agreed on 51\% of the tweets.
        \item \textbf{Disney}: All of us agreed on 34\% of the tweets.
\end{itemize}

For example, this means that out of the 100 Coca cola tweets, there were 42 which we all classified according to the same polarity while being either unsure or sure of the classification, and 58 where at least one member disagreed with the others while being either unsure or sure of the classification.

\section{Lexicon evaluation}

Using the results of our manual classification on tweets about Coca Cola, Disney and CostCo, we were able to evaluate the results of our unsupervised classification with the lexicon score on these three brands.
To classify a tweet with the lexicon, we get the positive and the negative score of this tweet, subtract one to the other, and finally apply an arbitrary threshold to get the classification.
This threshold was set to 2, so that if the score is greater than 2 the tweet is classified as positive, if the score is less than -2 the tweet is negative, and otherwise to tweet is classified neutral.

Here are the results we got on the 100 tweets we classified manually for each brand:
\begin{itemize}
        \item \textbf{Coca Cola}: the lexicon agrees on 53\% of the tweets.
        \item \textbf{CostCo}: the lexicon agrees on 58\% of the tweets.
        \item \textbf{Disney}: the lexicon agrees on 58\% of the tweets.
\end{itemize}

\section{SVM classification}

We evaluated our SVM classifier in much the same way as the lexicon, i e by using the results of our manual classification.
We used 450 tweets to train the SVM, based on the method previously described.

Here are the results we got on the 100 tweets we classified manually for each brand:
\begin{itemize}
        \item \textbf{Coca Cola}: the SVM agrees on 48\% of the tweets.
        \item \textbf{CostCo}: the SVM agrees on 50\% of the tweets.
        \item \textbf{Disney}: the SVM agrees on 16\% of the tweets.
\end{itemize}

\section{Final scores}
Here we can see the results achieved through the different methods by our program, compared to the poll:

\vspace{-0.6cm}
\centerline{\includegraphics[scale=0.45]{../img/full1.png}}
\vspace{-0.6cm}

The results of the SVM and lexicon were created by dividing the number of positive tweets with the number of all tweets excluding the neutral ones.

\chapter{Discussion}

\section{Lexicon}

The overall results from our classification using only the lexicons are far from great. 

From looking at how separate tweets were actually classified we noticed that the most common mistake the lexicons made was to classify a positive or negative tweet as neutral. 
This means that it is unable to detect the parts in these tweets that signal opinions, because they are not present in the lexicon, because they are too short, too ironic, or because they are actually contained in an emoticons or hashtag.
For instance, the following tweets are classified as neutral, even though they are actually positive:\\
\centerline{\textit{Costco Sundays!!}}\\
\centerline{\textit{Need to do a Costco run today... :) \#LoveThatStore}}

To reduce this problem one may add more lexicons and use lexicons from more sources. Six out of the seven lexicons we did use came from the same source. Those six were created by applying six different machine-learning techniques on the same initial corpus. That may have led to some issues such as giving too much importance to some words only because they were present in this initial dictionary.

Anoter measure to improve the result may be to make use of some Part-Of-Speech tagging, so the polarity of a word is evaluated using the context of the word. For example ``very good'' and ``not very good'' will more or less yield the same result. It would be good also to consider the position of the brand name, so that we can see the difference between ``Costco is great but I dislike ice-cream'' and ``I dislike Costco but I think ice-cream is great''.

We did perform some lemmatization but to improve the results one may make use of even more sophisticated lemmatization.

\section{Support Vector Machine}

Interestingly enough, we can see that the results from the SVM are below the ones from the lexicons.
This probably means that our SVM was not able to remove the noise in the training data induced by the lexicon method.
In the case of Coca Cola and CostCo, the results are pretty close to lexicon, with around 50\% of matching.
For Disney however, results are really bad. 
Our guess is that as Disney is a much more diversified company than the two others (they make movies, tv, run an amusement park, etc), the topics touched upon in those tweets are really different, thus the SVM would need a lot more learning datas to classify those tweets correctly.

If one looks closer at how the SVM actually classified separate tweets then we can see that unlike the lexicon it doesn't have a bias toward classifiying a tweet as neutral, instead it often classifies a negative tweet as positive and vice versa.
This in essence means that its mistakes are more serious.

We believe there are two major reasons for the poor performance of the SVM.

Firstly, using uni-grams and bi-grams as features to represent a tweet may not be enough. To get better results, one should probably add more relevant features, like for example information about the urls (number, domain name, etc), about the emoticons (number of positives, negatives) or about the tweet as a whole (length, number of verbs, number of adjectives etc).

Secondly, we trained our SVM with data that contained a lot of noise when we used the output of the lexicons to generate training data. The lexicons had a 50\% accuracy rate when classifying those tweets we manually classified, and this is probably way too noisy for training purpose.

\section{Poll}

The poll that was used was an aggregation of rankings done on three different subjects (“Feel good about”, “Admire and Respect” and “Trust”). The value of these should be quite strongly correlated among themselves and also correlated to whether the people who were interviewed actually “like” or “dislike” a brand. But we have no real scientific way of knowing.

We do know that the survey we used was exclusively performed in America. Unfortunately there is no way to filter tweets on the origin but one could at least filter it on language. To filter on english unfortunately does not exclude all non-americans (since it is such a global language). To make sure that the poll used is made on the same people that make the tweets one could use a poll conducted on people with a very local language and then filter on that language. The poll was conducted quite recently however.

However if one compares the poll with the manual classification the difference is on average only 3.77\%. The data in the manual classification is way too small to make any definitive conclusions but atleast it hints at the fact that the poll was quite accurate.

\section{Algorithm and features}

The computation of our algorithm ultimately comes from assumptions made on how a twitter user act to a tweet depending on the opinion of the user and the opinion voiced in a tweet. Our assumptions were that if the user re-tweets the tweet, favorites the tweet or is the follower of someone who made the tweet then the probability is higher that the user supports the opinion then otherwise. Intuitively this seems to be correct but judging from our results this seems not to be the case. The results instead rather indicate that at least one of these assumptions must be wrong since applying the algorithm seems to make the result worse, or rather one of the parameters seems to have an adverse effect. 

If one looks at how the different values influences the final ``influence-weight'' of the algorithm of the tweet we see that it is the re-tweets value that dominate the others. Given that our algorithm actually seem to make the results worse one may guess that people in practice in general do not re-tweet tweets they agree with, but rather tweets they disagree with. For example an environmentalist may mostly re-tweet statements he or she disagrees with to inform his or her followers of them, like new Gazprom oil discoveries.

Apart from the actual algorithm we were also really interested in developing the actionability of our analysis. There are several features that could have been applied to our solution in order to offer a wider range of tools for a company.

As we implemented the top ten influential tweets, we wanted to continue in the same direction and implement the top influential Twitter users as well. This feature would enable a company to easier reach out to its customers. We would have used a similar “influential weight” that what used for the influential tweets, but aggregating this measure by author instead.

Another extension to the current analysis would be to work with the links embedded in tweets. The idea is use the content of the links that are shared on twitter about a company, and output the most important ones. We may also use the contents of these pages to detect the polarity of a tweet.

\section{Conclusion}

In the end we conclude that our results are far from perfect and that more tests and a more sophisticated method is required to reach a definitive answer. However the results hints at using our influence algorithm actually makes the results worse. If this is true then this means that at least one of our assumptions when creating the algorithm was incorrect. A tweet with either more re-tweets, favorites or followers of its author actually reflects the opinion of the population less accurate then a tweet with less. Perhaps people tend to re-tweet tweets they find interesting, yet in effect have an opposite opinion.

\begin{thebibliography}{9}

%These references are very flexible to make however you want them, if you want them in a different format I'm open to suggestions

\bibitem{Twitter}
  Twitter
  \emph{The fastest, simplest way to stay close to everything you care about} (2013)
  \url{https://twitter.com/about} (2013/05/15)

\bibitem{SenWiki}
  Wikipedia
  \emph{Sentiment analysis} (2013)
  \url{http://en.wikipedia.org/wiki/Sentiment_analysis} (2013/05/15)

\bibitem{Pang02}
  Bo Pang, Lillian Lee and Shivakumar Vaithyanathan\\
  \emph{Thumbs up? Sentiment classification using machine learning techniques} (2002)\\
  \url{http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf} (2013/05/14)

\bibitem{Turney02}
  Peter Turney\\
  \emph{Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews} (2002)\\
  \url{http://arxiv.org/pdf/cs/0212032v1} (2013/05/14)

\bibitem{Taboada10}
  Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll and Manfred Stede\\
  \emph{Lexicon-Based Methods for Sentiment Analysis} (2010)\\
  \url{http://cgi.sfu.ca/~mtaboada/docs/Taboada_etal_SO-CAL.pdf} (2013/05/15)

\bibitem{WNOP07}
  Università degli studi di Pavia\\
  \emph{The MICRO-WNOP Corpus} (2007)\\
  \url{http://www.unipv.it/micrownop} (2013/05/15)
  
\bibitem{Liu04}
  Bing Liu and and Minqing Hu\\
  \emph{A list of positive and negative opinion words or sentiment words for English} (2004)
  \url{http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar} (2013/05/16)

\bibitem{NLTK}
  Natural Language Toolkit, a python plugin. Website:\\
  \url{http://nltk.org/}
  
\bibitem{Scikit}
  Scikit python plugin website:\\
  \url{http://scikit-learn.org/stable/}

\bibitem{Harris13}
  Harris Interactive\\
  \emph{The Harris Poll 2013 RQ® Summary Report} (2013)\\
  \url{http://www.harrisinteractive.com/vault/2013 RQ Summary Report FINAL.pdf} (2013/05/15)

\end{thebibliography}


\end{document}
